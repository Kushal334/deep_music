{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network for MNIST\n",
    "\n",
    "This notebook implements a (dynamic) multilayer recurrent neural network using long short term memory (LSTM) units and TensorFlow. It contains detailed explanations for each step.\n",
    "\n",
    "Author: Anna-Lena Popkes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation: truncated Backpropagation\n",
    "\n",
    "A recurrent neural network is designed in a way such that the output at a certain time step depends on arbitrarily distant inputs. In other words: when building an RNN in TensorFlow, the graph would have to be as wide as the input sequence.\n",
    "Unfortunately, this makes backpropagation computation both expensive and ineffective because gradients propagated over many time steps tend to either vanish (most of the time) or explode.\n",
    "\n",
    "A common solution to this problem is to create an \"unrolled\" version of the recurrent network that contains a fixed number (*n_steps*) of RNN inputs and outputs. In other words: backpropagation is \"truncated\" such that errors are only backpropagated for a fixed number of steps. A higher number of steps enables capturing long-term dependencies but is also more expensive (both regarding memory and computation).\n",
    "\n",
    "The model is then trained on this finite approximation of the recurrent network. Accordingly, at each time step the network is fed with inputs of length *n_steps*. The backward pass is performed after each input block. A short explanation is given on TensorFlow's [website](https://www.tensorflow.org/tutorials/recurrent#truncated-backpropagation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "eta = 0.01 # learning rate\n",
    "n_epochs = 4\n",
    "n_input = 28\n",
    "n_classes = 10\n",
    "batch_size = 100\n",
    "n_batches = mnist.train.images.shape[0]//batch_size\n",
    "\n",
    "# Network parameters\n",
    "n_hidden = 20 # number of hidden units per layer\n",
    "n_layers = 3 # number of layers \n",
    "n_steps = 28 # number of truncated backprop steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic vs. static RNN \n",
    "\n",
    "Tensorflow provides two RNN functions, namely *tf.nn.rnn* and *tf.nn.dynamic_rnn*. \n",
    "\n",
    "*tf.nn.rnn* creates an unrolled graph for a *fixed* RNN length. For example, when calling *tf.nn.rnn* with an input sequence of length 200, a static graph with 200 time steps is created. This has the disadvantage that we cannot feed longer or shorter sequences into the network than originally specified.\n",
    "\n",
    "*tf.nn.dynamic_rnn* solves this problem. It uses a tf.While loop to *dynamically* construct the graph when it's executed. This makes graph creation faster and allows for the input batches to vary in size.\n",
    "\n",
    "One difference between the two functions is the form of the input data. Whereas *tf.nn.rnn* takes a list of tensors as an input (namely a list of  n_steps tensors with shape (batch_size, input_size), *tf.nn.dynamic_rnn* takes as input the whole tensor of shape (batch_size, n_steps, input_size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture\n",
    "\n",
    "The basic architecture of a recurrent network looks as follows ([source](http://www.deeplearningbook.org/)). \n",
    "\n",
    "\n",
    "![title](figures/basic_rnn.png)\n",
    "\n",
    "\n",
    "\n",
    "As visibile in the figure, the state $h^{(t)}$  of the network depends both on the input $x^{(t)}$ and the previous state $h^{(t-1)}$.\n",
    "It is computed as follows:\n",
    "\n",
    "$$ h^{(t)} = \\sigma(U x^{(t)} + W h^{(t-1)} + b) $$\n",
    "\n",
    "with $\\sigma$ beign the $\\tanh$ in our implementation.\n",
    "\n",
    "\n",
    "The output is computed as: \n",
    "\n",
    "$$ o^{(t)} = V h^{(t)} + c $$\n",
    "$$ \\hat{y}^{(t)} = \\text{softmax}(o^{(t)}) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create placeholder variables for the input and targets\n",
    "X_placeholder = tf.placeholder(tf.float32, shape=[batch_size, n_steps, n_input])\n",
    "y_placeholder = tf.placeholder(tf.int32, shape=[batch_size, n_classes])\n",
    "\n",
    "# Create placeholder variables for final weight and bias matrix \n",
    "V = tf.Variable(tf.random_normal(shape=[n_hidden, n_classes]))\n",
    "c = tf.Variable(tf.random_normal(shape=[n_classes]))\n",
    "\n",
    "# For each initialized LSTM cell we need to specify how many hidden\n",
    "# units the cell should have.\n",
    "cell = tf.contrib.rnn.LSTMCell(num_units=n_hidden)\n",
    "\n",
    "# To create multiple layers we call the MultiRNNCell function that takes \n",
    "# a list of RNN cells as an input and wraps them into a single cell\n",
    "cell = tf.contrib.rnn.MultiRNNCell([cell]*n_layers)\n",
    "\n",
    "# Create a zero-filled state tensor as an initial state\n",
    "init_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "# Create a recurrent neural network specified by \"cell\", i.e. unroll the\n",
    "# network.\n",
    "# Returns a list of all previous RNN hidden states and the final states.\n",
    "# final_state contains n_layer LSTMStateTuple that contain both the \n",
    "# final hidden and the cell state of the respective layer.\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, \n",
    "                                         X_placeholder, \n",
    "                                         initial_state=init_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather final activations\n",
    "\n",
    "Because we are performing sequence *classification*, we are only interested in the output activations of the last timestep. Since tensorflow does not support negative indexing, we first transpose the tensor such that the \"n_steps\" axis is first. Then, we use tf.gather to select the correct slice. This process is illustrated below for the following parameter setting: batch_size=100, n_steps=28, n_hidden=20\n",
    "\n",
    "![title](figures/tensor_transformation.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = tf.transpose(outputs, [1,0,2])\n",
    "last_output = tf.gather(temp, int(temp.get_shape()[0]-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network output and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After gathering the final activations we can easily compute the logits\n",
    "# using a single matrix multiplication\n",
    "logits = tf.matmul(last_output, V)+c\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_placeholder,\n",
    "                                                           logits=logits))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(eta).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "We compute the accuracy of our model as follows.\n",
    "First, we use *tf.argmax* which gives us the highest entry in a tensor along some axis. For example, *tf.argmax(logits,1)* gives us the label our model considers to be most likely for each input. The true labels are computed using *tf.argmax(y_placeholder,1)*. In a next step, we compare these two tensors using *tf.equal* resulting in a tensor of boolean values.\n",
    "\n",
    "To compute the accuracy, we first cast the boolean values to floats using *tf.cast*. In a last step, we take the mean of all values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(y_placeholder,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training\n",
    "\n",
    "We train the network for the specified number of epochs. In each epoch, we run through all training examples, computing the loss and accuracy after 100 batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  0\n",
      "Minibatch loss: 2.35251  Accuracy: 0.18\n",
      "Minibatch loss: 0.504062  Accuracy: 0.85\n",
      "Minibatch loss: 0.324097  Accuracy: 0.92\n",
      "Minibatch loss: 0.194671  Accuracy: 0.92\n",
      "Minibatch loss: 0.0865606  Accuracy: 0.99\n",
      "Minibatch loss: 0.140195  Accuracy: 0.96\n",
      "\n",
      "Epoch:  1\n",
      "Minibatch loss: 0.124076  Accuracy: 0.97\n",
      "Minibatch loss: 0.0696919  Accuracy: 0.99\n",
      "Minibatch loss: 0.098697  Accuracy: 0.98\n",
      "Minibatch loss: 0.0694356  Accuracy: 0.99\n",
      "Minibatch loss: 0.0907172  Accuracy: 0.99\n",
      "Minibatch loss: 0.0110981  Accuracy: 1.0\n",
      "\n",
      "Epoch:  2\n",
      "Minibatch loss: 0.0895157  Accuracy: 0.98\n",
      "Minibatch loss: 0.0611437  Accuracy: 0.99\n",
      "Minibatch loss: 0.182449  Accuracy: 0.96\n",
      "Minibatch loss: 0.0999113  Accuracy: 0.97\n",
      "Minibatch loss: 0.0848051  Accuracy: 0.98\n",
      "Minibatch loss: 0.0522574  Accuracy: 0.99\n",
      "\n",
      "Epoch:  3\n",
      "Minibatch loss: 0.0710386  Accuracy: 0.99\n",
      "Minibatch loss: 0.0692343  Accuracy: 0.99\n",
      "Minibatch loss: 0.070002  Accuracy: 0.98\n",
      "Minibatch loss: 0.0168356  Accuracy: 1.0\n",
      "Minibatch loss: 0.0741237  Accuracy: 0.98\n",
      "Minibatch loss: 0.0583847  Accuracy: 0.98\n",
      "\n",
      "Optimization done! Let's calculate the test error\n",
      "\n",
      "Loss on test set:  0.0214669\n",
      "Accuracy on test set:  0.99\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    # We first have to initialize all variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Train for the specified number of epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        print()\n",
    "        print(\"Epoch: \", epoch)\n",
    "        \n",
    "        for batch in range(n_batches):\n",
    "            \n",
    "            x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            x_batch = x_batch.reshape((batch_size, n_steps, n_input))\n",
    "            \n",
    "            _train_step = sess.run(train_step, \n",
    "                                        feed_dict=\n",
    "                                        {X_placeholder:x_batch,\n",
    "                                         y_placeholder:y_batch\n",
    "                                        })\n",
    "            \n",
    "            \n",
    "            if batch%100 == 0:\n",
    "                _loss, _accuracy = sess.run([loss, accuracy],\n",
    "                                 feed_dict={\n",
    "                                     X_placeholder:x_batch,\n",
    "                                     y_placeholder:y_batch\n",
    "                                 })\n",
    "                print(\"Minibatch loss: %s  Accuracy: %s\" % (_loss, _accuracy))\n",
    "          \n",
    "    print()\n",
    "    print(\"Optimization done! Let's calculate the test error\")\n",
    "    \n",
    "    # Evaluate the model on the first \"batch_size\" test examples\n",
    "    x_test_batch, y_test_batch = mnist.test.next_batch(batch_size)\n",
    "    x_test_batch = x_test_batch.reshape((batch_size, n_steps, n_input))\n",
    "    \n",
    "    test_loss, test_accuracy, _train_step = sess.run([loss, accuracy, train_step],\n",
    "                                                    feed_dict={\n",
    "                                                        X_placeholder:x_test_batch,\n",
    "                                                        y_placeholder:y_test_batch\n",
    "                                                    })\n",
    "    print()\n",
    "    print(\"Loss on test set: \", test_loss)\n",
    "    print(\"Accuracy on test set: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
